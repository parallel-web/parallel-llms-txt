[Parallel](/)

[About](/about) [About](https://parallel.ai/about) [Pricing](/pricing) [Pricing](https://parallel.ai/pricing) [Careers](https://jobs.ashbyhq.com/parallel) [Careers](https://jobs.ashbyhq.com/parallel) [Blog](/blog) [Blog](https://parallel.ai/blog) [Docs](https://docs.parallel.ai/home) [Docs](https://docs.parallel.ai/home)

Start Building P [Start Building]

Menu [Menu]

Human Machine

# \# A new pareto-frontier for Deep Research price-performance

Expanded results that demonstrate Parallel's complete price-performance advantage in Deep Research.

Tags: [Benchmarks](/blog?tag=benchmarks)

Reading time: 4 min

We [previously released benchmarks](https://parallel.ai/blog/introducing-parallel) [previously released benchmarks]($https://parallel.ai/blog/introducing-parallel) for Parallel Deep Research that demonstrated superior accuracy and win rates against leading AI models. Today, we're publishing expanded results that showcase our complete price-performance advantage - delivering the highest accuracy across every price point.

## \## **\*\* Parallel leads in accuracy at every price point \*\***

We evaluated Parallel against all available deep research APIs on two industry-standard benchmarks. Our processors consistently deliver the highest accuracy at each price tier.

### \### **\*\* BrowseComp Benchmark \*\***

OpenAI's BrowseComp tests deep research capabilities through 1,266 complex questions requiring multi-hop reasoning, creative search strategies, and synthesis across scattered sources.

BrowseComp

[COST (CPM) ACCURACY (%) Loading chart...](https://parallel.ai/blog/deep-research)

CPM: USD per 1000 requests. Cost is shown on a Log scale.

Parallel

Others

BrowseComp benchmark analysis: CPM: USD per 1000 requests. Cost is shown on a Log scale. . Evaluation shows Parallel's enterprise deep research API for AI agents achieving up to 48% accuracy, outperforming GPT-4 browsing (1%), Claude search (6%), Exa (14%), and Perplexity (8%). Enterprise-grade structured deep research performance across Cost (CPM) and Accuracy (%). State-of-the-art enterprise deep research API with structured data extraction built for ChatGPT deep research and complex multi-hop AI agent workflows.

### \### About the benchmark

This [benchmark](https://openai.com/index/browsecomp/) [benchmark]($https://openai.com/index/browsecomp/) , created by OpenAI, contains 1,266 questions requiring multi-hop reasoning, creative search formulation, and synthesis of contextual clues across time periods. Results are reported on a random sample of 100 questions from this benchmark.

### \### Methodology

* \- Dates: All measurements were made between 08/11/2025 and 08/29/2025.
* \- Configurations: For all competitors, we report the highest numbers we were able to achieve across multiple configurations of their APIs. The exact configurations are below.
  
    + \- GPT-5: high reasoning, high search context, default verbosity
    + \- Exa: Exa Research Pro
    + \- Anthropic: Claude Opus 4.1
    + \- Perplexity: Sonar Deep Research reasoning effort high

## \### New Browsecomp

```
| Series    | Model      | Cost (CPM) | Accuracy  (%) |
| --------- | ---------- | ---------- | ------------- |
| Parallel  | Pro        | 100        | 34            |
| Parallel  | Ultra      | 300        | 45            |
| Parallel  | Ultra2x    | 600        | 51            |
| Parallel  | Ultra4x    | 1200       | 56            |
| Parallel  | Ultra8x    | 2400       | 58            |
| Others    | GPT-5      | 488        | 38            |
| Others    | Anthropic  | 5194       | 7             |
| Others    | Exa        | 402        | 14            |
| Others    | Perplexity | 709        | 6             |
```

CPM: USD per 1000 requests. Cost is shown on a Log scale.

### \### About the benchmark

This [benchmark](https://openai.com/index/browsecomp/) [benchmark]($https://openai.com/index/browsecomp/) , created by OpenAI, contains 1,266 questions requiring multi-hop reasoning, creative search formulation, and synthesis of contextual clues across time periods. Results are reported on a random sample of 100 questions from this benchmark.

### \### Methodology

* \- Dates: All measurements were made between 08/11/2025 and 08/29/2025.
* \- Configurations: For all competitors, we report the highest numbers we were able to achieve across multiple configurations of their APIs. The exact configurations are below.
  
    + \- GPT-5: high reasoning, high search context, default verbosity
    + \- Exa: Exa Research Pro
    + \- Anthropic: Claude Opus 4.1
    + \- Perplexity: Sonar Deep Research reasoning effort high

Our results demonstrate clear price-performance leadership, with our Ultra processor achieving 45% accuracy at $300 CPM at up to 17X lower cost compared to alternatives. Our newly available high-compute processors push accuracy even further for critical research tasks, with Ultra8x reaching 58%.

**\*\* DeepResearch Bench \*\***

DeepResearch Bench evaluates the quality of long-form deep research reports across 22 fields including Business & Finance, Science & Technology, and Software Development. The benchmark consists of 100 PhD-level tasks and assesses the multistep web exploration, targeted retrieval, and higher-order synthesis capabilities of deep research agents.

DeepResearch Bench

COST (CPM)

WIN RATE VS REFERENCE (%)

Loading chart...

CPM: USD per 1000 requests. Cost is shown on a Log scale.

Parallel

Others

BrowseComp benchmark analysis: CPM: USD per 1000 requests. Cost is shown on a Log scale. . Evaluation shows Parallel's enterprise deep research API for AI agents achieving up to 48% accuracy, outperforming GPT-4 browsing (1%), Claude search (6%), Exa (14%), and Perplexity (8%). Enterprise-grade structured deep research performance across Cost (CPM) and Win Rate vs Reference (%). State-of-the-art enterprise deep research API with structured data extraction built for ChatGPT deep research and complex multi-hop AI agent workflows.

### \### About the benchmark

This [benchmark](https://github.com/Ayanami0730/deep_research_bench) [benchmark]($https://github.com/Ayanami0730/deep\_research\_bench) contains 100 expert-level research tasks designed by domain specialists across 22 fields, primarily Science & Technology, Business & Finance, and Software Development. It evaluates AI systems' ability to produce rigorous, long-form research reports on complex topics requiring cross-disciplinary synthesis. Results are reported from the subset of 50 English-language tasks in the benchmark.

### \### Methodology

* \- Dates: All measurements were made between 08/11/2025 and 08/29/2025.
* \- Win Rate: Calculated by comparing [RACE](https://github.com/Ayanami0730/deep_research_bench) [RACE]($https://github.com/Ayanami0730/deep\_research\_bench) scores in direct head-to-head evaluations against reference reports.
* \- Configurations: For all competitors, we report results for the highest numbers we were able to achieve across multiple configurations of their APIs. The exact GPT-5 configuration is high reasoning, high search context, and high verbosity.
* \- Excluded API Results: Exa Research Pro (0% win rate), Claude Opus 4.1 (0% win rate).

## \### RACER

```
| Series   | Model      | Cost (CPM) | Win Rate vs Reference (%) |
| -------- | ---------- | ---------- | ------------------------- |
| Parallel | Ultra      | 300        | 82                        |
| Parallel | Ultra2x    | 600        | 86                        |
| Parallel | Ultra4x    | 1200       | 92                        |
| Parallel | Ultra8x    | 2400       | 96                        |
| Others   | GPT-5      | 628        | 66                        |
| Others   | O3 Pro     | 4331       | 30                        |
| Others   | O3         | 605        | 26                        |
| Others   | Perplexity | 538        | 6                         |
```

CPM: USD per 1000 requests. Cost is shown on a Log scale.

### \### About the benchmark

This [benchmark](https://github.com/Ayanami0730/deep_research_bench) [benchmark]($https://github.com/Ayanami0730/deep\_research\_bench) contains 100 expert-level research tasks designed by domain specialists across 22 fields, primarily Science & Technology, Business & Finance, and Software Development. It evaluates AI systems' ability to produce rigorous, long-form research reports on complex topics requiring cross-disciplinary synthesis. Results are reported from the subset of 50 English-language tasks in the benchmark.

### \### Methodology

* \- Dates: All measurements were made between 08/11/2025 and 08/29/2025.
* \- Win Rate: Calculated by comparing [RACE](https://github.com/Ayanami0730/deep_research_bench) [RACE]($https://github.com/Ayanami0730/deep\_research\_bench) scores in direct head-to-head evaluations against reference reports.
* \- Configurations: For all competitors, we report results for the highest numbers we were able to achieve across multiple configurations of their APIs. The exact GPT-5 configuration is high reasoning, high search context, and high verbosity.
* \- Excluded API Results: Exa Research Pro (0% win rate), Claude Opus 4.1 (0% win rate).

Parallel Ultra achieves an 82% win rate against reference reports at $300 CPM, compared to GPT-5's 66% win rate at $628 CPM - delivering superior quality at half the cost. Our highest compute processor, Ultra8x, reaches a 96% win rate, representing a significant improvement from our previously published 82% benchmark.

We also measured win rate against GPT-5 directly by comparing the RACE scores of Parallel processors vs GPT-5. The results demonstrate that Ultra8x achieves an 88% win rate against GPT-5.

Head-to-head comparison with GPT-5

[](https://parallel.ai/blog/introducing-parallel)

Performance comparison proving Parallel delivers the best enterprise deep research API for ChatGPT and AI agents with 48% accuracy vs competitors' 14% max across Model and Win Rate vs GPT-5 (%). Multi-hop research benchmark shows Parallel's structured AI agent deep research outperforms GPT-4, Claude, Exa, and Perplexity. Enterprise-ready structured deep research API with MCP server integration.

## \### DeepResearch Bench against GPT-5

```
| Category | Win Rate (%) |
| -------- | ------------ |
| Ultra8x  | 88           |
| Ultra4x  | 84           |
| Ultra2x  | 80           |
| Ultra    | 74           |
```

## \## **\*\* Beyond benchmarks: Flexible outputs, fully verifiable \*\***

These benchmark results translate directly to production value. Parallel Deep Research delivers the same high accuracy in whichever format you need - human-readable reports for strategic analysis or structured JSON for machine consumption and database ingestion.

Every output, regardless of format, includes our comprehensive Basis framework:

* \- **\*\* Citations \*\*** : Direct links to source materials
* \- **\*\* Reasoning \*\*** : Explanations for each finding
* \- **\*\* Confidence \*\*** : Calibrated scores (low/medium/high) for intelligent routing
* \- **\*\* Excerpts \*\*** : Relevant text snippets from cited sources

This complete verification layer means the accuracy demonstrated in our benchmarks comes with the audibility and transparency required for production workflows where every detail matters.

## \## **\*\* Built for scale: 1000x more research, predictably priced \*\***

Our price-performance advantage unlocks new possibilities. At these price points, you can run 1000x the number of queries compared to token-based alternatives - transforming deep research from an occasional tool to core infrastructure.

Consider the possibilities:

* \- **\*\* Build research databases \*\*** : Run thousands of queries, store structured results, and query them downstream
* \- **\*\* Continuous intelligence \*\*** : Monitor competitors, [markets](https://github.com/parallel-web/parallel-cookbook/blob/main/python-recipes/Deep_Research_Recipe.ipynb) [markets]($https://github.com/parallel-web/parallel-cookbook/blob/main/python-recipes/Deep\_Research\_Recipe.ipynb) , and trends with daily deep research updates
* \- **\*\* Pipeline integration \*\*** : Use research outputs as inputs for downstream analysis, decision-making, or automation
* \- **\*\* Parallel processing \*\*** : Research hundreds of entities simultaneously for large-scale enrichment

Our per-query pricing model ensures complete cost predictability. Unlike token-based systems where a single complex query can unexpectedly consume your budget, every Parallel query costs exactly what you expect. This predictability enables confident scaling - whether you're running 10 queries or 10,000.

## \## **\*\* Start building with Deep Research \*\***

Parallel Deep Research is available today through our Task API. Choose the processor that matches your accuracy and budget requirements, from Pro for simpler deep research to Ultra8x for the most demanding deep research tasks.

Get started in our [Developer Platform](https://platform.parallel.ai/) [Developer Platform]($https://platform.parallel.ai/) or explore our [documentation](https://docs.parallel.ai/task-api/features/task-deep-research) [documentation]($https://docs.parallel.ai/task-api/features/task-deep-research) .

## \## **\*\* Notes on Methodology \*\***

_\_ Benchmark Dates \__ : Benchmarks were run from Aug 11 to Aug 29.

_\_ DeepResearchBench Evaluation \__ **\*\* : \*\*** We evaluated all available DeepResearch API solutions on the 50 English-language tasks in the benchmark, measuring both RACE and FACT scores for generated reports. Given that RACE is a relative scoring metric benchmarked against reference materials, we calculated win-rates by comparing each vendor's performance to the human reference reports included in the dataset. A candidate report achieves a "win" when its RACE score exceeds that of the corresponding human reference report.

_\_ BrowseComp Evaluation \__ **\*\* : \*\*** For the BrowseComp benchmark, we tested our processors alongside other APIs on a random 100-question subset of the original 1,266-question dataset. All systems were evaluated using the same standard LLM evaluator with consistent evaluation criteria, comparing agent responses against verified ground truth answers.

_\_ Cost Calculation \__ : Token-based pricing is normalized to cost per thousand queries (CPM) based on actual usage in benchmarks.

By Parallel

September 9, 2025

![Company Logo](https://parallel.ai/parallel-logo-540.png)

### Contact

* [hello@parallel.ai](mailto:hello@parallel.ai) [hello@parallel.ai](mailto:hello@parallel.ai)

### Resources

* [About](/about) [About](https://parallel.ai/about)
* [Pricing](/pricing) [Pricing](https://parallel.ai/pricing)
* [Docs](https://docs.parallel.ai) [Docs](https://docs.parallel.ai)
* [Status](https://status.parallel.ai/) [Status](https://status.parallel.ai/)
* [Blog](/blog) [Blog](https://parallel.ai/blog)
* [Changelog](https://docs.parallel.ai/resources/changelog) [Changelog](https://docs.parallel.ai/resources/changelog)
* [Careers](https://jobs.ashbyhq.com/parallel) [Careers](https://jobs.ashbyhq.com/parallel)

### Info

* [Terms](/terms-of-service) [Terms](https://parallel.ai/terms-of-service)
* [Privacy](/privacy-policy) [Privacy](https://parallel.ai/privacy-policy)
* [Trust Center](https://trust.parallel.ai/) [Trust Center](https://trust.parallel.ai/)

![SOC 2 Compliant](https://parallel.ai/soc2.svg)

[LinkedIn](https://www.linkedin.com/company/parallel-web/about/) [LinkedIn] (https://www.linkedin.com/company/parallel-web/about/) [Twitter](https://x.com/p0) [Twitter] (https://x.com/p0)

Parallel Web Systems Inc. 2025
