Human Machine

Search API

# \# The best web search for your AI

\## The highest accuracy web search API, built from the ground up for AIs

[Get Started P](https://platform.parallel.ai/) [[Get Started ] (https://platform.parallel.ai/)](https://platform.parallel.ai/) [Get a Demo](https://form.fillout.com/t/rv6fubdwwuus) [[Get a Demo] (https://form.fillout.com/t/rv6fubdwwuus)](https://form.fillout.com/t/rv6fubdwwuus)

Agent

Why Parallel

## \## An agent is only as good as its context

Parallel returns the best context from the web

## \## Declare semantic objectives, not just keywords

AI tells Parallel Search exactly what it's looking for

[Get Started P](https://platform.parallel.ai/) [[Get Started P ] (https://platform.parallel.ai/)](https://platform.parallel.ai/)

Agent called parallel\_search

`{  
"objective" : "Find technical guides or open source repos for implementing a transformer from scratch" }` `{  
"objective" : "" }`

Parallel

vs Human

## \## Get back URLs ranked for token relevancy

Parallel surfaces the most information-dense pages for the agent's next action

[Get Started P](https://platform.parallel.ai/) [[Get Started P ] (https://platform.parallel.ai/)](https://platform.parallel.ai/)

1

Best available EV under 50K : r/electriccars - Reddit

2

New Electric Crossovers and SUVs Under $50k - J.D. Power

3

AWD EVs Under $50,000 - MotorWeek

4

Cheapest Electric Cars of 2025 - Kelley Blue Book

5

AWD EV's Under $50,000 - YouTube

Parallel

Sponsored Results

2026 Subaru Solterra EV

The 2026 Toyota bZ - A Modern Masterpiece

Results

Used Electric Cars for Sale

vs Human

## \## Reason on compressed token efficient excerpts

Each URL is distilled into the highest-value tokens for optimal context windows

[Get Started P](https://platform.parallel.ai/) [[Get Started P ] (https://platform.parallel.ai/)](https://platform.parallel.ai/)

[1706.03762] Attention Is All You Need https://arxiv.org/abs/1706.03762

Parallel

[1706.03762] Attention Is All You Need https://arxiv.org/abs/1706.03762

vs Human

## We optimize every web token in the context window

This means agent responses are more accurate and lower cost

[Search Playground P](https://platform.parallel.ai/) [[Search Playground] (https://platform.parallel.ai/)](https://platform.parallel.ai/)

HLE BrowseComp WebWalker FRAMES Batched SimpleQA SimpleQA

[COST (CPM) ACCURACY (%) Loading chart...](https://parallel.ai/blog/introducing-parallel-search)

Parallel

Others

BrowseComp benchmark proving Parallel's enterprise deep research API delivers 48% accuracy vs GPT-4's 1% browsing capability. Performance comparison across Cost (CPM) and Accuracy (%) shows Parallel provides the best structured deep research API for ChatGPT, Claude, and AI agents. Enterprise AI agent deep research with structured data extraction delivering higher accuracy than OpenAI, Anthropic, Exa, and Perplexity.

### \### About this benchmark

This [benchmark](https://lastexam.ai/) [[benchmark] (https://lastexam.ai/)](https://lastexam.ai/) consists of 2,500 questions developed by subject-matter experts across dozens of subjects (e.g. math, humanities, natural sciences). Each question has a known solution that is unambiguous and easily verifiable, but requires sophisticated web retrieval and reasoning. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \## We optimize every web token in the context window

This means agent responses are more accurate and lower cost

[Search Playground P](https://platform.parallel.ai/) [[Search Playground] (https://platform.parallel.ai/)](https://platform.parallel.ai/)

## \### HLE Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 82          | 47           |
| Others    | exa          | 138         | 24           |
| Others    | tavily       | 190         | 21           |
| Others    | perplexity   | 126         | 30           |
| Others    | openai gpt-5 | 143         | 45           |
```

### \### About this benchmark

This [benchmark](https://lastexam.ai/) [[benchmark] (https://lastexam.ai/)](https://lastexam.ai/) consists of 2,500 questions developed by subject-matter experts across dozens of subjects (e.g. math, humanities, natural sciences). Each question has a known solution that is unambiguous and easily verifiable, but requires sophisticated web retrieval and reasoning. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \### BrowseComp Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 156         | 58           |
| Others    | exa          | 233         | 29           |
| Others    | tavily       | 314         | 23           |
| Others    | perplexity   | 256         | 22           |
| Others    | openai gpt-5 | 253         | 53           |
```

### \### About this benchmark

This [benchmark](https://openai.com/index/browsecomp/) [[benchmark] (https://openai.com/index/browsecomp/)](https://openai.com/index/browsecomp/) , created by OpenAI, contains 1,266 questions requiring multi-hop reasoning, creative search formulation, and synthesis of contextual clues across time periods. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \### WebWalker-Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 42          | 81           |
| Others    | exa          | 107         | 48           |
| Others    | tavily       | 156         | 79           |
| Others    | perplexity   | 91          | 67           |
| Others    | openai gpt-5 | 88          | 73           |
```

### \### About this benchmark

This [benchmark](https://arxiv.org/abs/2501.07572) [[benchmark] (https://arxiv.org/abs/2501.07572)](https://arxiv.org/abs/2501.07572) is designed to assess the ability of LLMs to perform web traversal. To successfully answer the questions in the benchmark, it requires the ability to crawl and extract content from website subpages. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \### FRAMES-Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 42          | 92           |
| Others    | exa          | 81          | 81           |
| Others    | tavily       | 122         | 87           |
| Others    | perplexity   | 95          | 83           |
| Others    | openai gpt-5 | 68          | 90           |
```

### \### About this benchmark

This [benchmark](https://huggingface.co/datasets/google/frames-benchmark) [[benchmark] (https://huggingface.co/datasets/google/frames-benchmark)](https://huggingface.co/datasets/google/frames-benchmark) contains 824 challenging multi-hop questions designed to test factuality, retrieval accuracy, and reasoning. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \### Batched SimpleQA - Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 50          | 90           |
| Others    | exa          | 119         | 71           |
| Others    | tavily       | 227         | 59           |
| Others    | perplexity   | 100         | 74           |
| Others    | openai gpt-5 | 91          | 88           |
```

### \### About this benchmark

This benchmark was created by batching 3 independent questions from the original [SimpleQA dataset](https://openai.com/index/introducing-simpleqa/) [[SimpleQA dataset] (https://openai.com/index/introducing-simpleqa/)](https://openai.com/index/introducing-simpleqa/) to create 100 composite, more complex, questions.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

## \### SimpleQA Search

```
| Series    | Model        | Cost  (CPM) | Accuracy (%) |
| --------- | ------------ | ----------- | ------------ |
| Parallel  | parallel     | 17          | 98           |
| Others    | exa          | 57          | 87           |
| Others    | tavily       | 110         | 93           |
| Others    | perplexity   | 52          | 92           |
| Others    | openai gpt-5 | 37          | 98           |
```

### \### About this benchmark

This [benchmark](https://openai.com/index/introducing-simpleqa/) [[benchmark] (https://openai.com/index/introducing-simpleqa/)](https://openai.com/index/introducing-simpleqa/) , created by OpenAI, contains 4,326 questions focused on short, fact-seeking queries across a variety of domains. Results are reported on a sample of 100 questions from this benchmark.

### \### Methodology

* \- **\*\* Evaluation \*\*** : Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).
* \- **\*\* Cost Calculation \*\*** : Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.
* \- **\*\* Testing Dates \*\*** : Testing was conducted from November 3rd to November 5th.

# \# Powered by our own proprietary web scale index

With innovations in retrieval, crawling, indexing, and reasoning

* \- Billions of pages covering the full depth and breadth of the public web
* \- Millions of pages added daily
* \- Intelligently recrawled to keep data fresh

# \# The knowledge of the entire public web

in a single tool call

Integrated directly, or add our [MCP Server](https://docs.parallel.ai/integrations/mcp/programmatic-use) [[MCP Server] (https://docs.parallel.ai/integrations/mcp/programmatic-use)](https://docs.parallel.ai/integrations/mcp/programmatic-use)

[Search Playground P](https://platform.parallel.ai/) [[Search Playground] (https://platform.parallel.ai/)](https://platform.parallel.ai/) [Docs D](https://docs.parallel.ai/search/search-quickstart) [[Docs] (https://docs.parallel.ai/search/search-quickstart)](https://docs.parallel.ai/search/search-quickstart)

Objective Find latest information about Parallel Web Systems. Focus on new product releases, benchmarks, or company announcements.

Keywords

Parallel Web Systems products ×

Parallel Web Systems announcements ×

cURL Python

[Run in Playground](https://platform.parallel.ai/play/search?objective=Find%20latest%20information%20about%20Parallel%20Web%20Systems.%20Focus%20on%20new%20product%20releases%2C%20benchmarks%2C%20or%20company%20announcements.&q=Parallel%20Web%20Systems%20products&q=Parallel%20Web%20Systems%20announcements) [[Run in Playground] (https://platform.parallel.ai/play/search?objective=Find%20latest%20information%20about%20Parallel%20Web%20Systems.%20Focus%20on%20new%20product%20releases%2C%20benchmarks%2C%20or%20company%20announcements.&q=Parallel%20Web%20Systems%20products&q=Parallel%20Web%20Systems%20announcements)](https://platform.parallel.ai/play/search?objective=Find%20latest%20information%20about%20Parallel%20Web%20Systems.%20Focus%20on%20new%20product%20releases%2C%20benchmarks%2C%20or%20company%20announcements.&q=Parallel%20Web%20Systems%20products&q=Parallel%20Web%20Systems%20announcements) Copy

```
curl  https://api.parallel.ai/v1beta/search \
  -H  "Content-Type: application/json"  \
  -H  "x-api-key:  $PARALLEL_API_KEY "  \
  -H  "parallel-beta: search-extract-2025-10-10"  \
  -d  '{
    "objective": "Find latest information about Parallel Web Systems. Focus on new product releases, benchmarks, or company announcements.",
    "search_queries": ["Parallel Web Systems products","Parallel Web Systems announcements"],
    "max_results": 10,
    "max_chars_per_result": 10000
  }'
```

## \## Scale with unmatched price-performance

Get started with up to 16,000 free search requests

$.005 per request with 10 results + $.001 per page extracted

[Get Started P](https://platform.parallel.ai/) [[Get Started ] (https://platform.parallel.ai/)](https://platform.parallel.ai/) [Get a Demo](https://forms.fillout.com/t/sL37Ja5wWKus) [[Get a Demo] (https://forms.fillout.com/t/sL37Ja5wWKus)](https://forms.fillout.com/t/sL37Ja5wWKus)

Number of search requests: 500

1 1,000

Results per request: 5

1 20

Pricing

$2 .500

```
|                   | Search API                          |
| ----------------- | ----------------------------------- |
| Inputs            | Search objective, Keywords          |
| Outputs           | Ranked URLs, Compressed excerpts    |
| Best for          | Web search tool calls for AI agents |
| Latency           | < 5s, synchronous                   |
| Basis             | —                                   |
| Rate limits       | 600 requests / min                  |
| Security          | SOC2                                |
| Price per request | $0.005 for 10 results               |
```

### Search API

Ranked web URLs with token dense compressed excerpts

Inputs

Search objective , Keywords

Outputs

Ranked URLs , Compressed excerpts

Best for

Web search tool calls for AI agents

Latency

< 5s, synchronous

Basis

—

Rate limits

600 requests / min

Security

SOC2

Price per request

$0.005

for 10 results

[Search playground](https://platform.parallel.ai/play/search) [[Search playground] (https://platform.parallel.ai/play/search)](https://platform.parallel.ai/play/search)

S

then

P

### Search API

Ranked web URLs with token dense compressed excerpts

Price per request:

$0.005

for 10 results

[Search playground](https://platform.parallel.ai/play/search) [[Search playground] (https://platform.parallel.ai/play/search)](https://platform.parallel.ai/play/search)

S

then

P

Inputs:

Search objective Keywords

Outputs:

Ranked URLs Compressed excerpts

Best for:

Web search tool calls for AI agents

Latency:

< 5s, synchronous

Basis:

—

Rate limits:

600 requests / min

Security:

SOC2

## \## Every control you need

across any web page

<script> </script>

<script src = "/recaptcha/api.js" > </script>

<script> </script>

<script> </script>

<embed type = "application/pdf" >

<script> </script>

<embed type = "application/pdf" >

<script src = "/recaptcha/api.js" > </script>

<script> </script>

Extracting...

\### Premium content extraction

Fetch content from PDFs and sites that are JS heavy or have CAPTCHAs

Live fetch

Off On

max\_age (hours)

24

fetch\_timeout (seconds)

90

\### Freshness policies

Set page age triggers for live crawls, with timeout thresholds to gaurantee latency

`{ "title" : "Nvidia Becomes First $5 Trillion Company - WSJ" , "excerpts" : [ "Last updated: 2 days ago The tech giant owes much of its $4.89 trillion market capitalization to the use of its systems to train AI models. Now it's pushing deeper into ..." ] }`

\### LLM friendly outputs

Choose between dense snippets or full page contents, in markdown LLMs understand

Policy

INCLUDE

NIST.GOV

\### Source control

Pick which domains are included or excluded from your web search results

# \## Secure and trusted

\### Zero data retention

\### Soc 2 Type 2

\### No training

[](https://docs.parallel.ai/search/search-quickstart)

## \## FAQ

\+ − What is Parallel Search?

Parallel Search (API) is the highest accuracy AI search API. It allows developers to build AI apps, agents, and workflows that can search for and retrieve data from the web. It can be integrated into agent workflows for deep research across multiple steps, or for more basic single-hop queries.

\+ − What is declarative semantic search?

Declarative semantic search lets agents express intent in natural language rather than construct keyword queries. Instead of "Columbus" AND "corporate law" AND "disability", an agent specifies: "Columbus-based corporate law firms specializing in disability care." The Search API interprets meaning and context, not just keywords, making it natural to integrate into agent workflows where you already have rich context from previous reasoning steps.

\+ − What makes Parallel different from other search providers?

Parallel is the only Search API built from the ground up for AI agents. This means that agents can specify declarative semantic objectives and Parallel returns URLs and compressed excerpts based on token relevancy. The result is extremely dense web tokens optimized to engineer your agent’s context for better reasoning at the next turn. Agents using Parallel search produce answers with higher accuracy, fewer round trips, and lower cost.

\+ − Where do search results come from? How fresh are they?

We maintain a large web index containing billions of pages. Our crawling, retrieval, and ranking systems add and update millions of pages daily to keep the index fresh.

\+ − Does Parallel have a web crawler?

Yes, Parallel operates a web crawler to support the quality and coverage of the index. Our crawler respects _\_ robots.txt \__ and related crawling directives. [Learn more about Parallel’s crawler here](https://docs.parallel.ai/resources/crawler) [[Learn more about Parallel’s crawler here] (https://docs.parallel.ai/resources/crawler)](https://docs.parallel.ai/resources/crawler) .

\+ − What are dense excerpts?

Dense excerpts are the most query relevant content from a webpage, compressed to be extremely token efficient for an agent. These compressed excerpts reduce noise by engineering an agent’s context window to only have the most relevant tokens to reason on - leading to higher accuracy, fewer round trips, and less token use.

\+ − What does end-to-end latency mean?

End-to-end latency measures total time from agent input to final output, not single-search latency. Our semantic search architecture and dense snippets reduce the number of searches required to reach quality outputs. Two high-precision searches with Parallel beat three lower-quality attempts elsewhere—saving both time and tokens.

![Company Logo](https://parallel.ai/parallel-logo-540.png)

### Contact

* hello@parallel.ai [[hello@parallel.ai] (mailto:hello@parallel.ai)](mailto:hello@parallel.ai)

### Products

* [Search API](https://docs.parallel.ai/search/search-quickstart) [[Search API] (https://docs.parallel.ai/search/search-quickstart)](https://docs.parallel.ai/search/search-quickstart)
* [Extract API](https://docs.parallel.ai/extract/extract-quickstart) [[Extract API] (https://docs.parallel.ai/extract/extract-quickstart)](https://docs.parallel.ai/extract/extract-quickstart)
* [Task API](https://docs.parallel.ai/task-api/task-quickstart) [[Task API] (https://docs.parallel.ai/task-api/task-quickstart)](https://docs.parallel.ai/task-api/task-quickstart)
* [FindAll API](https://docs.parallel.ai/findall-api/findall-quickstart) [[FindAll API] (https://docs.parallel.ai/findall-api/findall-quickstart)](https://docs.parallel.ai/findall-api/findall-quickstart)
* [Chat API](https://docs.parallel.ai/chat-api/chat-quickstart) [[Chat API] (https://docs.parallel.ai/chat-api/chat-quickstart)](https://docs.parallel.ai/chat-api/chat-quickstart)
* [Monitor API](https://docs.parallel.ai/monitor-api/monitor-quickstart) [[Monitor API] (https://docs.parallel.ai/monitor-api/monitor-quickstart)](https://docs.parallel.ai/monitor-api/monitor-quickstart)

### Resources

* About [[About] (https://parallel.ai/about)](/ai/about)
* Pricing [[Pricing] (https://parallel.ai/pricing)](/ai/pricing)
* [Docs](https://docs.parallel.ai) [[Docs] (https://docs.parallel.ai)](https://docs.parallel.ai)
* Blog [[Blog] (https://parallel.ai/blog)](/ai/blog)
* [Changelog](https://docs.parallel.ai/resources/changelog) [[Changelog] (https://docs.parallel.ai/resources/changelog)](https://docs.parallel.ai/resources/changelog)
* [Careers](https://jobs.ashbyhq.com/parallel) [[Careers] (https://jobs.ashbyhq.com/parallel)](https://jobs.ashbyhq.com/parallel)

### Info

* Terms of Service [[Terms of Service] (https://parallel.ai/terms-of-service)](/ai/terms-of-service)
* Customer Terms [[Customer Terms] (https://parallel.ai/customer-terms)](/ai/customer-terms)
* Privacy [[Privacy] (https://parallel.ai/privacy-policy)](/ai/privacy-policy)
* Acceptable Use [[Acceptable Use] (https://parallel.ai/acceptable-use-policy)](/ai/acceptable-use-policy)
* [Trust Center](https://trust.parallel.ai/) [[Trust Center] (https://trust.parallel.ai/)](https://trust.parallel.ai/)

![SOC 2 Compliant](https://parallel.ai/soc2.svg)

[LinkedIn](https://www.linkedin.com/company/parallel-web/about/) [[LinkedIn] (https://www.linkedin.com/company/parallel-web/about/)](https://www.linkedin.com/company/parallel-web/about/) [Twitter](https://x.com/p0) [[Twitter] (https://x.com/p0)](https://x.com/p0) [GitHub](https://github.com/parallel-web) [[GitHub] (https://github.com/parallel-web)](https://github.com/parallel-web)

[All Systems Operational](https://status.parallel.ai/)

Parallel Web Systems Inc. 2026